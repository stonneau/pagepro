@article{Tonneau2014,
title = "Using task efficient contact configurations to animate creatures in arbitrary environments ",
journal = "Computers & Graphics ",
volume = "",
number = "0",
pages = " - ",
year = "2014",
note = "",
issn = "0097-8493",
doi = "http://dx.doi.org/10.1016/j.cag.2014.08.005",
url = "http://www.sciencedirect.com/science/article/pii/S009784931400079X",
author = "Steve Tonneau and Julien Pettr√© and Franck Multon",
keywords = "Autonomous virtual characters",
keywords = "Animation for games",
keywords = "Procedural Animation",
keywords = "Contact Before Motion",
keywords = "Range Of Motion",
keywords = "Force Transmission Ratio ",
abstract = "Abstract A common issue in three-dimensional animation is the creation of contacts between a virtual creature and the environment. Contacts allow force exertion, which produces motion. This paper addresses the problem of computing contact configurations allowing to perform motion tasks such as getting up from a sofa, pushing an object or climbing. We propose a two-step method to generate contact configurations suitable for such tasks. The first step is an offline sampling of the range of motion (ROM) of a virtual creature. The \{ROM\} of the human arms and legs is precisely determined experimentally. The second step is a run time request confronting the samples with the current environment. The best contact configurations are then selected according to a heuristic for task efficiency. The heuristic is inspired by the force transmission ratio. Given a contact configuration, it measures the potential force that can be exerted in a given direction. The contact configurations are then used as inputs for an inverse kinematics solver that will compute the final animation. Our method is automatic and does not require examples or motion capture data. It is suitable for real time applications and applies to arbitrary creatures in arbitrary environments. Various scenarios (such as climbing, crawling, getting up, pushing or pulling objects) are used to demonstrate that our method enhances motion autonomy and interactivity in constrained environments. "
}

